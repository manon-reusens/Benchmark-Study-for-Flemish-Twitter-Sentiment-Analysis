{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import copy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_annotations=  pd.read_csv('C:/Users/u0146965/OneDrive - KU Leuven/Sentiment analysis/full_cleaned_dataset.csv')\n",
    "df_annotations.head() # to display the first 5 lines of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_set(dataset_column):\n",
    "    y= dataset_column\n",
    "    y = y.replace('NEUTRAL','1')\n",
    "    y = y.replace('POSITIVE','2')\n",
    "    y = y.replace('NEGATIVE','0')\n",
    "    for i in y:\n",
    "        i=int(i)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_mat(y_test, y_pred):\n",
    "    mat=confusion_matrix(y_test, y_pred)/len(y_test)\n",
    "    return sns.heatmap(mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_feat --> sqrt, log2, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X, y_inputkolom, max_feat, n_gram, n_est=100, max_feat_rf='sqrt' ):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    #do TFIDF\n",
    "    vect=TfidfVectorizer(max_features=max_feat, stop_words= stopwords.words('dutch'), token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b', ngram_range=n_gram).fit(X_train)\n",
    "    X_train= vect.transform(X_train)\n",
    "    X_test= vect.transform(X_test)\n",
    "    X_train=pd.DataFrame(X_train.toarray(), columns= vect.get_feature_names())\n",
    "    X_test=pd.DataFrame(X_test.toarray(), columns= vect.get_feature_names())\n",
    "    \n",
    "    # Build a Random Forest\n",
    "    model = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = model.predict(X_test)\n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "        \n",
    "    print('The Accuracy of the model is', model.score(X_test, y_test))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    \n",
    "    \n",
    "    #return y_predict, y_test, log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_val(X,y_inputkolom, max_feat, n_gram, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "    vect=TfidfVectorizer(max_features=max_feat, stop_words= stopwords.words('dutch'), token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b', ngram_range=n_gram).fit(X_train)\n",
    "    X_train= vect.transform(X_train)\n",
    "    X_val= vect.transform(X_val)\n",
    "    X_train=pd.DataFrame(X_train.toarray(), columns= vect.get_feature_names())\n",
    "    X_val=pd.DataFrame(X_val.toarray(), columns= vect.get_feature_names())\n",
    "   \n",
    "    # Build a Random Forest\n",
    "    model = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = model.predict(X_val)\n",
    "    acc_score= model.score(X_val, y_val)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[1000, 2000, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    acc_score=rf_val(df_annotations['text'], df_annotations['label'], i, (1,1))\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2)]:\n",
    "    acc_score=rf_val(df_annotations['text'], df_annotations['label'],8000 , i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing the number of trees and the number of considered features by each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['text'], df_annotations['label'],8000 , (1,1) , i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['text'], df_annotations['label'],8000 , (1,1) , 200,  i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('number of estimaters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF and then rf with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf(df_annotations['text'], df_annotations['label'], 8000, (1,1), 200, 'log2' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[1000, 2000, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    acc_score=rf_val(df_annotations['processed_annotations_joined'].astype('U'), df_annotations['label'], i, (1,1))\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2)]:\n",
    "    rf_val(df_annotations['processed_annotations_joined'].astype('U'), df_annotations['label'],6000 , i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing the regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['processed_annotations_joined'].astype('U'), df_annotations['label'],6000 , (1,1) , i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['processed_annotations_joined'].astype('U'), df_annotations['label'],6000 , (1,1) , 150,  i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('number of estimaters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF with optimal parameters with extra preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf(df_annotations['processed_annotations_joined'].astype('U'), df_annotations['label'], 6000, (1,1), 150 ,'log2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[1000, 2000, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    acc_score=rf_val(df_annotations['processed_annotations_lemmatized_joined'].astype('U'), df_annotations['label'],i , (1,1))\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2)]:\n",
    "    rf_val(df_annotations['processed_annotations_lemmatized_joined'].astype('U'), df_annotations['label'],12000, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['processed_annotations_lemmatized_joined'].astype('U'), df_annotations['label'], 12000, (1,2),i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['processed_annotations_lemmatized_joined'].astype('U'), df_annotations['label'],12000 , (1,2) , 150,  i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf(df_annotations['processed_annotations_lemmatized_joined'].astype('U'), df_annotations['label'], 12000, (1,2), 150 , 'log2' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec selftrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(data_column):\n",
    "    sentences=[]\n",
    "    for i in data_column:\n",
    "        if type(i)!=float:\n",
    "            words_in_sentence=i.split()\n",
    "            sentences.append(words_in_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_val(X,y_inputkolom, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "    model= Word2Vec(sentences=sent(X_train), vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_train:\n",
    "        l=[]\n",
    "        for word in i.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                l.append(list(model.wv.get_vector(word)))\n",
    "            else:\n",
    "                l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        #a = np.array(l)\n",
    "        #print(a)\n",
    "        #res = np.average(a, axis=0)\n",
    "        #lijst=list(res)\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_train=pd.DataFrame(Word2Vec_text_av)\n",
    "    \n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_val:\n",
    "        l=[]\n",
    "        for word in i.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                l.append(list(model.wv.get_vector(word)))\n",
    "            else:\n",
    "                l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        #a = np.array(l)\n",
    "        #print(a)\n",
    "        #res = np.average(a, axis=0)\n",
    "        #lijst=list(res)\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_val=pd.DataFrame(Word2Vec_text_av)\n",
    "   \n",
    "    # Build a logistic regression\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = rf.predict(X_val)\n",
    "    acc_score= rf.score(X_val, y_val)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X,y_inputkolom, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    model= Word2Vec(sentences=sent(X_train), vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_train:\n",
    "        l=[]\n",
    "        for word in i.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                l.append(list(model.wv.get_vector(word)))\n",
    "            else:\n",
    "                l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_train=pd.DataFrame(Word2Vec_text_av)\n",
    "    \n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_test:\n",
    "        l=[]\n",
    "        for word in i.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                l.append(list(model.wv.get_vector(word)))\n",
    "            else:\n",
    "                l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_test=pd.DataFrame(Word2Vec_text_av)\n",
    "    \n",
    "    # Build a logistic regression\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = rf.predict(X_test)\n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "        \n",
    "    print('The Accuracy of the model is', rf.score(X_test, y_test))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    \n",
    "    \n",
    "    #return y_predict, y_test, log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we choose vector_size= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20]\n",
    "for i in wind:\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], 300,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje) #wind=5, min_c=1, work=4, sg=1, alpha= 0.025,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we kiezen window 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20]\n",
    "for i in min_c:\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], 300,15,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dus we kiezen min_count 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], 300,15,5,4,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dus sgd=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], 300,15,5,16,1,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus we kiezen 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], 300,15,5,16,1,0.05,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val(df_annotations['text'],df_annotations['label'], 300,15,5,16,1,0.05,150 ,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf(df_annotations['text'],df_annotations['label'], 300,15,5,16,1,0.05,150 ,'sqrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_val_lem(X,y_inputkolom, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "    model= Word2Vec(sentences=sent(X_train), vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_train:\n",
    "        l=[]\n",
    "        if type(i)== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in i.split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_train=pd.DataFrame(Word2Vec_text_av)\n",
    "    \n",
    "    \n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_val:\n",
    "        l=[]\n",
    "        if type(i)== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in i.split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "    \n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_val=pd.DataFrame(Word2Vec_text_av)\n",
    "   \n",
    "    # Build a logistic regression\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = rf.predict(X_val)\n",
    "    acc_score= rf.score(X_val, y_val)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_lem(X,y_inputkolom, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    model= Word2Vec(sentences=sent(X_train), vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_train:\n",
    "        l=[]\n",
    "        if type(i)== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in i.split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_train=pd.DataFrame(Word2Vec_text_av)\n",
    "    \n",
    "    \n",
    "    Word2Vec_text_av=[]\n",
    "    for i in X_test:\n",
    "        l=[]\n",
    "        if type(i)== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in i.split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "    \n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X_test=pd.DataFrame(Word2Vec_text_av)\n",
    "   \n",
    "    # Build a logistic regression\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels \n",
    "    y_predict = rf.predict(X_test)\n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "        \n",
    "    print('The Accuracy of the model is', rf.score(X_test, y_test))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20]\n",
    "for i in wind:\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20]\n",
    "for i in min_c:\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,15,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,15 ,7 ,16,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,15 , 7,16,1,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,15,7,16,1,0.1,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    acc_score=rf_val_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,15,7,16,1,0.1,200 ,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_lem(df_annotations['processed_annotations_lemmatized_joined'],df_annotations['label'], 100,15,7,16,1,0.1,200 ,'sqrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/u0146965/OneDrive - KU Leuven/Sentiment analysis/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_pretrained_emb(data):\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(data)):\n",
    "        l=[]\n",
    "        \n",
    "        if type(data[i])== float:\n",
    "            l.append([0]*300)\n",
    "        else:\n",
    "            for word in data[i].split():\n",
    "                if word in model.key_to_index:\n",
    "                    l.append(list(model.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*300)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def rf(X, y_inputkolom, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Build a logistic regression\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = rf.predict(X_test)\n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Micro F1 measure is', metrics.f1_score(new, y_pred, average='micro'))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Micro precsion is',metrics.precision_score(new, y_pred, average='micro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Micro recall is',metrics.recall_score(new, y_pred, average='micro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "        \n",
    "    print('The Accuracy of the model is', rf.score(X_test, y_test))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    \n",
    "    \n",
    "    #return y_predict, y_test, log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_val(X,y_inputkolom, n_est=100, max_feat_rf='sqrt'):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "\n",
    "    # Build a logistic regression\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_features= max_feat_rf).fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = rf.predict(X_val)\n",
    "    acc_score= rf.score(X_val, y_val)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V_pretrained_emb(df_annotations['text'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V_pretrained_emb(df_annotations['text'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], 150 ,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['text'])\n",
    "acc_score=rf(X, df_annotations['label'], 150,'sqrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], 200 ,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "acc_score=rf(X, df_annotations['label'], 200, 'sqrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fasttext.cc/docs/en/support.html#building-fasttext-python-module\n",
    "#https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext.util.download_model('nl', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('C:/Users/u0146965/OneDrive - KU Leuven/Sentiment analysis/cc.nl.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_pretrained_emb(data):\n",
    "    fasttext_text_av=[]\n",
    "    for i in range(len(data)):\n",
    "        l=[]\n",
    "        \n",
    "        if type(data[i])== float:\n",
    "            l.append([0]*300)\n",
    "        else:\n",
    "            for word in data[i].split():\n",
    "                l.append(list(model.get_word_vector(word)))\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        fasttext_text_av.append(avg)\n",
    "    X=pd.DataFrame(fasttext_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], 150 ,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "acc_score=rf(X, df_annotations['label'], 150, 'sqrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=[10, 50, 100, 150, 200]\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('regularisation term')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(n_est, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "n_est=['sqrt', 'log2']\n",
    "for i in n_est:\n",
    "    print('the regularisation term is', i)\n",
    "    X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "    acc_score=rf_val(X,df_annotations['label'], 200 ,i)\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "acc_score=rf(X, df_annotations['label'], 200,'sqrt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
