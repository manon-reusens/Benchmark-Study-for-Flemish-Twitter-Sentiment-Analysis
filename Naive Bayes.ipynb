{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20652342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import copy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations=  pd.read_csv('full_cleaned_dataset.csv')\n",
    "df_annotations.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(data, max_feat, n_gram):\n",
    "    vect=TfidfVectorizer(max_features=max_feat, stop_words= stopwords.words('dutch'), token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b', ngram_range=n_gram).fit(data)\n",
    "    X= vect.fit_transform(data)\n",
    "    X=pd.DataFrame(X.toarray(), columns= vect.get_feature_names())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7af158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_set(dataset_column):\n",
    "    y= dataset_column\n",
    "    y = y.replace('NEUTRAL','1')\n",
    "    y = y.replace('POSITIVE','2')\n",
    "    y = y.replace('NEGATIVE','0')\n",
    "    for i in y:\n",
    "        i=int(i)\n",
    "    new=[]\n",
    "    for i in y:\n",
    "        new.append(int(i))\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ed6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_mat(y_test, y_pred):\n",
    "    mat=confusion_matrix(y_test, y_pred)/len(y_test)\n",
    "    return sns.heatmap(mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNaiveBayes(X, y_inputkolom):\n",
    "    y= y_set(y_inputkolom)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Build a logistic regression\n",
    "    model = MultinomialNB()\n",
    "    #model = ComplementNB()\n",
    "    #model = GaussianNB()\n",
    "    #model = BernoulliNB()\n",
    "    mnb=model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = mnb.predict(X_test)\n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "    \n",
    "    print('The Accuracy of the model is', accuracy_score(y_test, y_predict))\n",
    "    print('The Spearman correlation of the model is ', stats.spearmanr(y_test, y_predict))\n",
    "    print('The Kendall tau of the model is ', stats.kendalltau(y_test,y_predict))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    #return y_predict, y_test, log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45731ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BNaiveBayes(X, y_inputkolom, binary= 0.0):\n",
    "    y= y_set(y_inputkolom)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Build a Naive Bayes model\n",
    "    model = BernoulliNB(binarize= binary)\n",
    "    mnb=model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = mnb.predict(X_test)\n",
    "    \n",
    "    new=[]\n",
    "    for i in y_test:\n",
    "        new.append(int(i))\n",
    "        \n",
    "    y_pred=[]\n",
    "    for i in y_predict:\n",
    "        y_pred.append(int(i))\n",
    "    print(' The Macro F1 measure is', metrics.f1_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro precision is',metrics.precision_score(new, y_pred, average='macro'))\n",
    "    print(' The Macro recall is',metrics.recall_score(new, y_pred, average='macro'))\n",
    "    \n",
    "    print('The Accuracy of the model is', accuracy_score(y_test, y_predict))\n",
    "    print('The Spearman correlation of the model is ', stats.spearmanr(y_test, y_predict))\n",
    "    print('The Kendall tau of the model is ', stats.kendalltau(y_test,y_predict))\n",
    "    print('The confusion matrix looks as follows', cf_mat(y_test, y_predict))\n",
    "    #return y_predict, y_test, log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNaiveBayes_val_tuning(X,y_inputkolom):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "\n",
    "    # Build a logistic regression\n",
    "    model = MultinomialNB()\n",
    "    mnb=model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = mnb.predict(X_val)\n",
    "    acc_score= accuracy_score(y_val, y_predict)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BNaiveBayes_val_tuning(X,y_inputkolom, binary=0.0):\n",
    "    y= y_set(y_inputkolom)\n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "\n",
    "    # Build a logistic regression\n",
    "    model = BernoulliNB(binarize=binary)\n",
    "    cnb=model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels \n",
    "    y_predict = cnb.predict(X_val)\n",
    "    acc_score= accuracy_score(y_val, y_predict)\n",
    "    print('The Accuracy of the model is', acc_score)\n",
    "    #print('The confusion matrix looks as follows', cf_mat(y_val, y_predict))\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c2a2d",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ddd55",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c420c2",
   "metadata": {},
   "source": [
    "### Plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a38f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.text.values.astype('U'),i,(1,1))\n",
    "    acc_score=MNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336aec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74acc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2), (2,2), (1,3), (2,3), (3,3)]:\n",
    "    X=TFIDF(df_annotations.text.values.astype('U'),8000,i)\n",
    "    MNaiveBayes_val_tuning(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b66c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.text.astype('U'), 8000,(1,1))\n",
    "MNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0fdce",
   "metadata": {},
   "source": [
    "### Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7ed95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),i,(1,1))\n",
    "    acc_score=MNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16fc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2), (2,2), (1,3), (2,3), (3,3)]:\n",
    "    X=TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),10000,i)\n",
    "    MNaiveBayes_val_tuning(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.processed_annotations_joined.astype('U'), 10000,(1,1))\n",
    "MNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b1ce7",
   "metadata": {},
   "source": [
    "### Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a04a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000, 15000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.values.astype('U'),i,(1,1))\n",
    "    acc_score=MNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925e394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2), (2,2), (1,3), (2,3), (3,3)]:\n",
    "    X=TFIDF(df_annotations.processed_annotations_lemmatized_joined.values.astype('U'),12000,i)\n",
    "    MNaiveBayes_val_tuning(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac64890",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.astype('U'), 12000,(1,2))\n",
    "MNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c306d7",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(data_column):\n",
    "    sentences=[]\n",
    "    for i in range(len(df_annotations['text'])):\n",
    "        words_in_sentence=df_annotations['text'][i].split()\n",
    "        sentences.append(words_in_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V(sent, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025):\n",
    "    model= Word2Vec(sentences=sent, vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(df_annotations['text'])):\n",
    "        l=[]\n",
    "        for word in df_annotations['text'][i].split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                l.append(list(model.wv.get_vector(word)))\n",
    "            else:\n",
    "                l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e391b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723eaf0",
   "metadata": {},
   "source": [
    "### Plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000,1200,1500,1800,2000,2500,3000]\n",
    "for i in max_vect:\n",
    "    X=W2V(sent(df_annotations['text']), i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fed34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20]\n",
    "for i in wind:\n",
    "    X=W2V(sent(df_annotations['text']), 600, i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76428070",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [ 20, 25, 30,35,40,45,50]\n",
    "for i in wind:\n",
    "    X=W2V(sent(df_annotations['text']), 600, i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96466449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710dc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20,30,50]\n",
    "for i in min_c:\n",
    "    X=W2V(sent(df_annotations['text']), 600, 40,i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    X=W2V(sent(df_annotations['text']), 600, 40,3,16,i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a557078",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    X=W2V(sent(df_annotations['text']), 600, 40,3,16,1,i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e19bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860d215",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=W2V(sent(df_annotations['text']), 600, 40,4,16,1,0.05)\n",
    "MNaiveBayes(normalize(X), df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c8c8a",
   "metadata": {},
   "source": [
    "### Lemmatized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408acf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_lem(sent, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025):\n",
    "    model= Word2Vec(sentences=sent, vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(df_annotations['processed_annotations_lemmatized_joined'])):\n",
    "        l=[]\n",
    "        if type(df_annotations['processed_annotations_lemmatized_joined'][i])== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in df_annotations['processed_annotations_lemmatized_joined'][i].split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9544c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d812e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20,25,30,35,40,45,50]\n",
    "for i in wind:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38107fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20,30,50]\n",
    "for i in min_c:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 35,i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7351abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 35,2,16,i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 35,2,16,1,i)\n",
    "    acc_score=MNaiveBayes_val_tuning(normalize(X), df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 35,2,16,1,0.025)\n",
    "MNaiveBayes(normalize(X), df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbdfa5c",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eecdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_pretrained_emb(data):\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(data)):\n",
    "        l=[]\n",
    "        \n",
    "        if type(data[i])== float:\n",
    "            l.append([0]*300)\n",
    "        else:\n",
    "            for word in data[i].split():\n",
    "                if word in model.key_to_index:\n",
    "                    l.append(list(model.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*300)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ba642",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['text'])\n",
    "MNaiveBayes(normalize(X), df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "MNaiveBayes(normalize(X), df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb5f99",
   "metadata": {},
   "source": [
    "## Pretrained Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b956b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('cc.nl.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_pretrained_emb(data):\n",
    "    fasttext_text_av=[]\n",
    "    for i in range(len(data)):\n",
    "        l=[]\n",
    "        \n",
    "        if type(data[i])== float:\n",
    "            l.append([0]*300)\n",
    "        else:\n",
    "            for word in data[i].split():\n",
    "                l.append(list(model.get_word_vector(word)))\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        fasttext_text_av.append(avg)\n",
    "    X=pd.DataFrame(fasttext_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fa7a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "MNaiveBayes(normalize(X), df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fe891",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "MNaiveBayes(normalize(X), df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cafdd5",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820a6f1",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e89da0",
   "metadata": {},
   "source": [
    "### Plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb9d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000,15000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.text.values.astype('U'),i,(1,1))\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722777a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83d5fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2), (2,2), (1,3), (2,3), (3,3)]:\n",
    "    X=TFIDF(df_annotations.text.values.astype('U'),8000,i)\n",
    "    BNaiveBayes_val_tuning(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dbc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.text.astype('U'), 8000,(1,1))\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2a710",
   "metadata": {},
   "source": [
    "### Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92c666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000,13000,14000,15000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),i,(1,1))\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41c722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2), (2,2), (1,3), (2,3), (3,3)]:\n",
    "    X=TFIDF(df_annotations.processed_annotations_joined.values.astype('U'),12000,i)\n",
    "    BNaiveBayes_val_tuning(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.processed_annotations_joined.astype('U'), 12000,(1,1))\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb28983",
   "metadata": {},
   "source": [
    "### Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113e375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_feat_list=[100,300,500,750 ,1000, 1100, 1500, 2000, 2500, 3000, 4000,5000 , 6000, 8000, 10000, 12000,13000,14000,15000,16000]\n",
    "for i in max_feat_list:\n",
    "    print('the number of max_features is', i)\n",
    "    X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.values.astype('U'),i,(1,1))\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_feat_list, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c887f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [(1,1),(1,2), (2,2), (1,3), (2,3), (3,3)]:\n",
    "    X=TFIDF(df_annotations.processed_annotations_lemmatized_joined.values.astype('U'),12000,i)\n",
    "    BNaiveBayes_val_tuning(X,df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a9f87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X= TFIDF(df_annotations.processed_annotations_lemmatized_joined.astype('U'), 12000,(1,2))\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec3307",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3933f7",
   "metadata": {},
   "source": [
    "### Plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    X=W2V(sent(df_annotations['text']), i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20,25,30,35,40,45,50]\n",
    "for i in wind:\n",
    "    X=W2V(sent(df_annotations['text']), 500, i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [50,55,60,65,70,75,80,85,90,100]\n",
    "for i in wind:\n",
    "    X=W2V(sent(df_annotations['text']), 500, i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "for i in min_c:\n",
    "    X=W2V(sent(df_annotations['text']), 500, 65,i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ce69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    X=W2V(sent(df_annotations['text']), 500, 65,6,16,i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    X=W2V(sent(df_annotations['text']), 500, 56,6,16,1,i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a944a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=W2V(sent(df_annotations['text']), 500, 56,6,16,1,0.05)\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a805f",
   "metadata": {},
   "source": [
    "### Lemmatized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W2V_lem(sent, vector_s, wind=5, min_c=1, work=4, sg=1, alpha= 0.025):\n",
    "    model= Word2Vec(sentences=sent, vector_size=vector_s, window=wind, min_count=min_c, workers=work, sg=sg, alpha=alpha)\n",
    "    Word2Vec_text_av=[]\n",
    "    for i in range(len(df_annotations['processed_annotations_lemmatized_joined'])):\n",
    "        l=[]\n",
    "        if type(df_annotations['processed_annotations_lemmatized_joined'][i])== float:\n",
    "            l.append([0]*vector_s)\n",
    "        else:\n",
    "            for word in df_annotations['processed_annotations_lemmatized_joined'][i].split():\n",
    "                if word in model.wv.key_to_index:\n",
    "                    l.append(list(model.wv.get_vector(word)))\n",
    "                else:\n",
    "                    l.append([0]*vector_s)\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*l)]\n",
    "        Word2Vec_text_av.append(avg)\n",
    "    X=pd.DataFrame(Word2Vec_text_av)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "max_vect= [50,100,200,300,500,600,700,1000]\n",
    "for i in max_vect:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(max_vect, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "wind= [1,2,3,4,5,6,7,8,9,10, 12, 15, 20]\n",
    "for i in wind:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(wind, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "min_c= [0,1,2,3,4,5,6,7,8,9,10,20,30,50]\n",
    "for i in min_c:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 15,i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('min_count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(min_c, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "for i in [0,1]:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 15,7,16,i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotje=[]\n",
    "alphas= [0.01,0.025,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1]\n",
    "for i in alphas:\n",
    "    X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 15,7,16,1,i)\n",
    "    acc_score=BNaiveBayes_val_tuning(X, df_annotations['label'])\n",
    "    plotje.append(acc_score)\n",
    "    print(acc_score)\n",
    "print(plotje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(alphas, plotje)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff82463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_lem(sent(df_annotations['processed_annotations_lemmatized_joined']), 600, 15,7,16,1,0.1)\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129591a",
   "metadata": {},
   "source": [
    "## Word2Vec pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f678527",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['text'])\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=W2V_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d711cb",
   "metadata": {},
   "source": [
    "## Pretrained Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474d229",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['text'])\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fasttext_pretrained_emb(df_annotations['processed_annotations_lemmatized_joined'])\n",
    "BNaiveBayes(X, df_annotations['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728996e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
