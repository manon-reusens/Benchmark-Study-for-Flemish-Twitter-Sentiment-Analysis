{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb393d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_annotations=  pd.read_csv('full_cleaned_dataset.csv')\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "predictions=[]\n",
    "for i in range(len(df_annotations['text'])):\n",
    "    uitkomst={}\n",
    "    uitkomst= analyzer.polarity_scores(df_annotations['text'][i])\n",
    "    predictions.append(uitkomst)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"predictions_VADER.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(predictions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checken dat de eerste voorspelling gelijk is aan wat ik hieronder heb gecheckt\n",
    "# namelijk {'neg': 0.0, 'neu': 0.599, 'pos': 0.401, 'compound': 0.8111}\n",
    "for i in predictions:\n",
    "    test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa68ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"predictions_VADER.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    del test[i]['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for i in range(len(test)):\n",
    "    max_key = max(test[i], key=test[i].get)\n",
    "    pred.append(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e08d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(len(test)):\n",
    "    lijst_a=[]\n",
    "    lijst_a=list(test[i].values())\n",
    "    if lijst_a.count( test[i][max(test[i], key=test[i].get)])>1:\n",
    "        #index_not_sure.append(i)\n",
    "        max_value= test[i][max(test[i], key=test[i].get)]\n",
    "        not_sure=[]\n",
    "        for sleutel in test[i].keys():\n",
    "            if test[i][sleutel]==max_value:\n",
    "                not_sure.append(sleutel)    \n",
    "        print(not_sure)    \n",
    "        print(i)\n",
    "        print(test[i])\n",
    "        if len(not_sure)!=0:\n",
    "            print(not_sure)\n",
    "            pred[i]=random.choice(not_sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    pred[i] = pred[i].replace('neu','NEUTRAL')\n",
    "    pred[i] = pred[i].replace('pos','POSITIVE')\n",
    "    pred[i] = pred[i].replace('neg','NEGATIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#definieer de confusion matrix\n",
    "def cf_mat(y_test, y_pred):\n",
    "    #ax= plt.subplot()\n",
    "    mat=confusion_matrix(y_test, y_pred, labels= ['NEGATIVE', 'NEUTRAL', 'POSITIVE']\n",
    "    )/len(y_test)\n",
    "    #ax.xaxis.set_ticklabels(['NEGATIVE', 'NEUTRAL', 'POSITIVE']); ax.yaxis.set_ticklabels(['POSITIVE', 'NEUTRAL','NEGATIVE' ]);\n",
    "    return sns.heatmap(mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(accuracy_score(df_annotations['label'], pred))\n",
    "print(cf_mat(df_annotations['label'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    pred[i] = pred[i].replace('NEUTRAL', '1')\n",
    "    pred[i] = pred[i].replace('POSITIVE', '2')\n",
    "    pred[i] = pred[i].replace('NEGATIVE', '0') \n",
    "for i in range(len(pred)):\n",
    "    pred[i]= int(pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905aff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zet de y-waarden goed\n",
    "y= df_annotations['label']\n",
    "y = y.replace('NEUTRAL','1')\n",
    "y = y.replace('POSITIVE','2')\n",
    "y = y.replace('NEGATIVE','0')\n",
    "for i in range(len(y)):\n",
    "    y[i]=int(y[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a43c8",
   "metadata": {},
   "source": [
    "F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08523789",
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[]\n",
    "for i in y:\n",
    "    new.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.f1_score(new, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13aff17",
   "metadata": {},
   "source": [
    "Precision and recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(new, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(new, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f36e32",
   "metadata": {},
   "source": [
    "# VADER Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9cca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "pred_lem=[]\n",
    "for i in range(len(df_annotations['processed_annotations_lemmatized_joined'])):\n",
    "    #print(i)\n",
    "    uitkomst={}\n",
    "    uitkomst= analyzer.polarity_scores(df_annotations['processed_annotations_lemmatized_joined'][i])\n",
    "    pred_lem.append(uitkomst)\n",
    "    #time.sleep(9)\n",
    "print(pred_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pred_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c97be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    del test[i]['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f63fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for i in range(len(test)):\n",
    "    max_key = max(test[i], key=test[i].get)\n",
    "    pred.append(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daff918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(len(test)):\n",
    "    lijst_a=[]\n",
    "    lijst_a=list(test[i].values())\n",
    "    if lijst_a.count( test[i][max(test[i], key=test[i].get)])>1:\n",
    "        #index_not_sure.append(i)\n",
    "        max_value= test[i][max(test[i], key=test[i].get)]\n",
    "        not_sure=[]\n",
    "        for sleutel in test[i].keys():\n",
    "            if test[i][sleutel]==max_value:\n",
    "                not_sure.append(sleutel)    \n",
    "        print(not_sure)    \n",
    "        print(i)\n",
    "        print(test[i])\n",
    "        if len(not_sure)!=0:\n",
    "            print(not_sure)\n",
    "            pred[i]=random.choice(not_sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    pred[i] = pred[i].replace('neu','NEUTRAL')\n",
    "    pred[i] = pred[i].replace('pos','POSITIVE')\n",
    "    pred[i] = pred[i].replace('neg','NEGATIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02188ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#definieer de confusion matrix\n",
    "def cf_mat(y_test, y_pred):\n",
    "    #ax= plt.subplot()\n",
    "    mat=confusion_matrix(y_test, y_pred, labels= ['NEGATIVE', 'NEUTRAL', 'POSITIVE']\n",
    "    )/len(y_test)\n",
    "    #ax.xaxis.set_ticklabels(['NEGATIVE', 'NEUTRAL', 'POSITIVE']); ax.yaxis.set_ticklabels(['POSITIVE', 'NEUTRAL','NEGATIVE' ]);\n",
    "    return sns.heatmap(mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c97e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(accuracy_score(df_annotations['label'], pred))\n",
    "print(cf_mat(df_annotations['label'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred)):\n",
    "    pred[i] = pred[i].replace('NEUTRAL', '1')\n",
    "    pred[i] = pred[i].replace('POSITIVE', '2')\n",
    "    pred[i] = pred[i].replace('NEGATIVE', '0') \n",
    "for i in range(len(pred)):\n",
    "    pred[i]= int(pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aefabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zet de y-waarden goed\n",
    "y= df_annotations['label']\n",
    "y = y.replace('NEUTRAL','1')\n",
    "y = y.replace('POSITIVE','2')\n",
    "y = y.replace('NEGATIVE','0')\n",
    "for i in range(len(y)):\n",
    "    y[i]=int(y[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b069b",
   "metadata": {},
   "source": [
    "F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04652114",
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[]\n",
    "for i in y:\n",
    "    new.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.f1_score(new, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa574a",
   "metadata": {},
   "source": [
    "Precision and recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(new, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(new, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d6072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
